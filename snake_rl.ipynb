{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfeb67d-1a92-4ab9-8ffd-ef7b6e866515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from torchrl.objectives import ClipPPOLoss\n",
    "from torchrl.objectives.value import GAE\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de65a09c-63ff-41ea-b5a4-76e02bc44382",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame:\n",
    "    def __init__(self, board_size=40):\n",
    "        self.board_size = board_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=int)\n",
    "        self.snake = [(self.board_size // 2, self.board_size // 2)]\n",
    "        self.board[self.snake[0]] = -1\n",
    "        self.direction = (0, 1)  #Start moving to the right\n",
    "        self.spawn_food()\n",
    "        self.done = False\n",
    "        return self.board\n",
    "\n",
    "    def spawn_food(self):\n",
    "        empty_spaces = np.argwhere(self.board == 0)\n",
    "        if len(empty_spaces) == 0:\n",
    "            self.done = True\n",
    "            return\n",
    "        food_position = random.choice(empty_spaces)\n",
    "        self.board[tuple(food_position)] = 1\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Game is over. Please reset the game.\")\n",
    "\n",
    "        # 0: up, 1: right, 2: down, 3: left\n",
    "        directions = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        self.direction = directions[action]\n",
    "\n",
    "        new_head = (self.snake[0][0] + self.direction[0], self.snake[0][1] + self.direction[1])\n",
    "\n",
    "        # Check if the new head is out of bounds or hits the snake body\n",
    "        if (new_head[0] < 0 or new_head[0] >= self.board_size or\n",
    "            new_head[1] < 0 or new_head[1] >= self.board_size or\n",
    "            self.board[new_head] == -1):\n",
    "            self.done = True\n",
    "            return deepcopy(self.board), -1, deepcopy(self.done), {}  # Game over (minus reward)\n",
    "\n",
    "        # Check if the new head is on the food\n",
    "        if self.board[new_head] == 1:\n",
    "            self.snake.insert(0, new_head)  # Add new head to the snake, first idx in list of snake body points\n",
    "            self.board[new_head] = -1\n",
    "            self.spawn_food() \n",
    "            return deepcopy(self.board), 1, deepcopy(self.done), {}  # Reward for eating food\n",
    "\n",
    "        # Move the snake\n",
    "        self.snake.insert(0, new_head)\n",
    "        self.board[new_head] = -1\n",
    "        tail = self.snake.pop()\n",
    "        self.board[tail] = 0\n",
    "\n",
    "        return deepcopy(self.board), 0, deepcopy(self.done), {}  # Normal step with no reward\n",
    "\n",
    "    def render(self):\n",
    "        return deepcopy(self.board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2625ef2-9ffd-4a49-bb88-6f598c0570e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False, True, False, False, False]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#game setup test\n",
    "\n",
    "game = SnakeGame()\n",
    "game.reset()\n",
    "\n",
    "list_imgs = []\n",
    "# #testing\n",
    "actions = [1, 2, 3, 0]  \n",
    "for i in range(100):\n",
    "    board, reward, done, _ = game.step(actions[i % 4])\n",
    "    list_imgs.append(board)\n",
    "    if done:\n",
    "        print(\"Game Over\")\n",
    "        break\n",
    "\n",
    "truth_arr = [np.all(x == list_imgs[0]) for x in list_imgs]\n",
    "print(truth_arr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(list_imgs[0])\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(list_imgs[frame])\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(list_imgs), interval=1000, blit=True)\n",
    "\n",
    "# saves the animation in our desktop \n",
    "#ani.save('snaketest.gif', writer = 'Pillow', fps = 30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6efb0-90da-4f56-9b5d-f845126d1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup policy \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=4, kernel_size=3, stride=1)       \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 4, out_channels = 8, kernel_size = 3, stride = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1)\n",
    "        \n",
    "        # one dropout layer\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        # one dense layer\n",
    "        self.final_dense = nn.Linear(11200, output_dim) \n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm2d(4)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(16)\n",
    "\n",
    "        for m in (self.conv1, self.conv2, self.conv3, self.final_dense):   #converts from default weight normalization to glorot\n",
    "            nn.init.constant_(m.bias,0)\n",
    "            nn.init.xavier_normal_(m.weight, np.sqrt(2))\n",
    "\n",
    "    # forward-pass function\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))     #after each convolution, pool and normalize\n",
    "        x = self.pool(x)            #used to reduce dimensionality\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        \n",
    "#         x = relu(self.conv4(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm4(x)\n",
    "        \n",
    "#         x = relu(self.conv5(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm5(x)\n",
    "        \n",
    "        x = x.reshape(len(x),-1)\n",
    "        x = self.dropout(x)\n",
    "        x = softmax(self.final_dense(x), dim = 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "input_dim = (40,40)\n",
    "output_dim = 4  # Four possible actions: up, down, left, right\n",
    "\n",
    "policy_net = PolicyNetwork(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c2af-1391-4a53-8e28-64f6463f1f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup critic\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=4, kernel_size=3, stride=1)       \n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 4, out_channels = 8, kernel_size = 3, stride = 1)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = 3, stride = 1)\n",
    "        \n",
    "        # one dropout layer\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        # one dense layer\n",
    "        self.final_dense = nn.Linear(11200, output_dim) \n",
    "\n",
    "        self.batchnorm1 = nn.BatchNorm2d(4)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(16)\n",
    "\n",
    "        for m in (self.conv1, self.conv2, self.conv3, self.final_dense):   #converts from default weight normalization to glorot(xavier)\n",
    "            nn.init.constant_(m.bias,0)\n",
    "            nn.init.xavier_normal_(m.weight, np.sqrt(2))\n",
    "\n",
    "    # forward-pass function\n",
    "    def forward(self, x):\n",
    "        x = relu(self.conv1(x))     #after each convolution, pool and normalize\n",
    "        x = self.pool(x)            #used to reduce dimensionality\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        \n",
    "#         x = relu(self.conv4(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm4(x)\n",
    "        \n",
    "#         x = relu(self.conv5(x))\n",
    "#         x = self.pool(x)\n",
    "#         x = self.batchnorm5(x)\n",
    "        \n",
    "        x = x.reshape(len(x),-1)\n",
    "        x = self.dropout(x)\n",
    "        x = softmax(self.final_dense(x), dim = 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "input_dim = (40,40)\n",
    "output_dim = 1  # one ouput, the predicted value of the state\n",
    "\n",
    "value_net = CriticNetwork(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029176c3-4831-4e36-a8eb-f3ca75f1c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy_net,\n",
    "    critic_network=value_net,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_bonus=True,\n",
    "    entropy_coef=0.005,\n",
    "    # these keys match by default but we set this for completeness\n",
    "    critic_coef=1.0,\n",
    "    loss_critic_type=\"smooth_l1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a35a2-03b7-4de6-9caf-fd82504eefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(env, policy_net, critic_net, gamma):\n",
    "    #should return\n",
    "    \n",
    "    #current state\n",
    "    #action chosen at that state\n",
    "    #log probabilites of actions at this current state, from which the action was chosen\n",
    "    #dones: a flag indicating whether the end of an episode\n",
    "    #reward: reward for taking the action chosen\n",
    "    #return: sum of all future rewards\n",
    "    #gamma: fixed discount factor for future rewards\n",
    "\n",
    "    state_list = []\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        #get action probabilities from the policy network\n",
    "        logits = policy_net(state)\n",
    "        action_probs = torch.softmax(logits, dim=1).squeeze(0).detach().numpy()\n",
    "\n",
    "        #choose action\n",
    "        action = np.random.choice(len(action_probs), p=action_probs)\n",
    "        \n",
    "        #step env\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        #save trajectory information\n",
    "        state_value = critic_net(state).item()\n",
    "        log_prob = torch.log(torch.tensor(action_probs[action]))\n",
    "\n",
    "        state_list.append([state, action, log_prob, done, reward, state_value])\n",
    "        state = next_state\n",
    "\n",
    "    # Calculate returns and advantages\n",
    "    get_returns(state_list, gamma)\n",
    "    get_advantages(state_list)\n",
    "    return state_list\n",
    "\n",
    "def get_returns(state_list, gamma):\n",
    "    #for each state and associated data in list, sum all subsequent rewards til fin\n",
    "    \n",
    "    for i in range(len(state_list)):\n",
    "        gamma_disc = 1\n",
    "        #state_list[i][-2] initialized to 0\n",
    "        for y in range(len(state_list)-i):\n",
    "            gamma_disc *= gamma\n",
    "            state_list[i][-2] += gamma_disc * state_list[y+i+1][4]\n",
    "def get_advantages(state_list):\n",
    "    for state in state_list:\n",
    "        advantage = state[-1] - state[5]  # Return - Value\n",
    "        state.append(advantage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08666b3a-a574-4885-a27a-1ed9181372ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78251970",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "gamma = 0.99\n",
    "clip_epsilon = 0.2\n",
    "\n",
    "env = SnakeGame()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    trajectories = get_train_data(env, policy_net, value_net, gamma)\n",
    "\n",
    "    states = torch.cat([s[0] for s in trajectories])\n",
    "    actions = torch.tensor([s[1] for s in trajectories], dtype=torch.int64)\n",
    "    log_probs_old = torch.cat([s[2] for s in trajectories])\n",
    "    returns = torch.tensor([s[3] for s in trajectories], dtype=torch.float32)\n",
    "    advantages = torch.tensor([s[4] for s in trajectories], dtype=torch.float32)\n",
    "\n",
    "    #update policy (actor)\n",
    "    logits = policy_net(states)\n",
    "    log_probs_new = torch.log_softmax(logits, dim=1).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "    #update value function (critic)\n",
    "    values = value_net(states).squeeze(1)\n",
    "    critic_loss = nn.functional.smooth_l1_loss(values, returns)\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "\n",
    "    #non-manual, use this\n",
    "    # # Compute loss using ClipPPOLoss\n",
    "    # loss = loss_module(\n",
    "    #     states=states,\n",
    "    #     actions=actions,\n",
    "    #     logprobs=log_probs_old,\n",
    "    #     returns=returns,\n",
    "    #     advantages=advantages,\n",
    "    # )\n",
    "\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    value_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220bb470-e349-4876-8334-c2cc44382bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after completing all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c6000-b0b0-46f9-b5d3-9cbbd2c84979",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = SnakeGame()\n",
    "game.reset()\n",
    "\n",
    "list_imgs = []\n",
    "# #testing\n",
    "done = false\n",
    "total_reward = 0\n",
    "while done == false:\n",
    "    board, reward, done, _ = game.step(value_net(game.render())\n",
    "    list_imgs.append(board)\n",
    "    reward += reward\n",
    "\n",
    "print(\"Game terminated w reward :\" + reward)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(list_imgs[0])\n",
    "\n",
    "def update(frame):\n",
    "    im.set_array(list_imgs[frame])\n",
    "    return [im]\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=len(list_imgs), interval=1000, blit=True)\n",
    "\n",
    "# saves the animation in our desktop \n",
    "ani.save('snaketest.gif', writer = 'Pillow', fps = 30) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
